{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data From NHTSA GES Repository Years 2010-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differing data files and formats across years, as well as storage directories made it difficult to obtain data using a batch iteration directly from the file transfer protocol set up by NHTSA. Directories for specific years were downloaded manually. Due to complex variation of feature column names, value categories and filenames, the attribute names, years and filepaths were first stored in data containers for ease in building a data cleaning script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories storing all sas files needed for each year in NHTSA GES dataset\n",
    "dir_years = [r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\GES10_PCSAS\\repost GES\", \n",
    "             r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\GES11_PCSAS\", \n",
    "             r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\GES12_PCSAS\", \n",
    "             r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\GES13_PCSAS\", \n",
    "             r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\GES2014SAS\", \n",
    "             r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\GES2015sas\"]\n",
    "# store dictionary of attributes to extract from each sas file type: person, accident, vehicle, vevent\n",
    "sas_file_dict = {\n",
    "              \"person\": \n",
    "              {\"cols\" :['VEHNO', 'VEH_NO', 'CASENUM', 'PER_TYP', 'AGE', 'SEX', 'ALC_RES', 'ALTRSULT']}, \n",
    "             \"accident\": \n",
    "              {\"cols\": ['CASENUM', 'MONTH', 'DAY_WEEK', 'YEAR', 'HOUR', 'MINUTE', 'LAND_USE',\n",
    "                        'RELJCT2', 'REL_ROAD', 'WRK_ZONE',\n",
    "              'INT_HWY', 'TYP_INT', 'NON_INVL', 'MAN_COL',\n",
    "              'MAX_SEV', 'MAN_COLL', 'PERNOTMVIT']}, \n",
    "             \"vehicle\" :\n",
    "              {\"cols\" : ['VEHNO', 'VEH_NO', 'CASENUM', 'TRAV_SP', 'VSPD_LIM', 'ACC_TYPE', 'MAKE', 'MODEL',\n",
    "               'MOD_YEAR', 'MODEL_YR', 'DR_ZIP', 'VNUM_LAN', 'NUMOCCS', 'DZIPCODE']},\n",
    "             \"vevent\": {\"cols\" : ['CASENUM', 'AOI1', 'GAD', 'VEHNO', 'VEH_NO']}\n",
    "                }\n",
    "\n",
    "# range of years stored in list \n",
    "years = list(range(2010,2016))\n",
    "# store data first by file type (person, accident, vehicle, vevent) then by year \n",
    "for file in sas_file_dict.keys():\n",
    "    sas_file_dict[file]['years'] = (dict(zip(years, dir_years)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create a dataframe with Distracted Driving Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store dataframes of sas files containing distracted driving data\n",
    "distracted_dfs = []\n",
    "# iterate through years and their respective file directories\n",
    "for year, file in zip(years, dir_years):\n",
    "    global df\n",
    "    df = pd.read_sas(os.path.join(file, \"distract.sas7bdat\"))\n",
    "    if year == 2010:\n",
    "        df = df.rename(columns = {\"VEHNO\": \"VEH_NO\"})\n",
    "    df[\"VEH_NO\"] = df[\"VEH_NO\"].astype(str)\n",
    "    df[\"CASENUM\"] = df[\"CASENUM\"].astype(str)\n",
    "    # create unique identifier that creates a row for each car involved in\n",
    "    # any crash\n",
    "    df[\"CASENUM_VEH_NO\"] = df[\"CASENUM\"] + df[\"VEH_NO\"]\n",
    "    df = df.drop_duplicates(subset = \"CASENUM_VEH_NO\", keep = \"first\")\n",
    "    distracted_dfs.append(df)\n",
    "\n",
    "# concatenate the dataframes across years\n",
    "global distracted_df\n",
    "distracted_df = pd.concat(distracted_dfs)\n",
    "distracted_dfs.clear()\n",
    "distracted_df = distracted_df.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring together NHTSA GES crash data across years and file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update current_df during iteration\n",
    "global current_df\n",
    "# concat years of each file type (person, accident, event, )\n",
    "concat_file_type = []\n",
    "# iterate through filetype and respective columns in sas_file_dict\n",
    "for file_type, file_type_attrs in sas_file_dict.items():\n",
    "    # iterate through each year \n",
    "    for year, file_list in (file_type_attrs['years'].items()):\n",
    "        filepath = os.path.join(file_list, (file_type + \".sas7bdat\"))\n",
    "        # access columns specific to year and file type\n",
    "        columns = file_type_attrs['cols']\n",
    "        # set mutable variable to adjust columns as necessary for year and file type\n",
    "        global columns_adj\n",
    "        # make a copy of columns \n",
    "        columns_adj = (columns.copy())\n",
    "        # adjust columns as necessary for year and file type\n",
    "        if year == 2010 and file_type != \"accident\": \n",
    "            columns_adj.remove(\"VEH_NO\")\n",
    "        if year ==  2010 and file_type == \"accident\":\n",
    "            columns_adj.remove('MAN_COLL')\n",
    "            columns_adj.remove('PERNOTMVIT')\n",
    "        if year ==  2010 and file_type == \"vevent\":\n",
    "            columns_adj.remove(\"AOI1\")\n",
    "        if year ==  2010 and file_type == \"person\":\n",
    "            columns_adj.remove('ALC_RES')\n",
    "        if year ==  2010 and file_type == \"vehicle\":\n",
    "            columns_adj.remove(\"MOD_YEAR\")\n",
    "            columns_adj.remove(\"DR_ZIP\")\n",
    "        if year !=  2010 and file_type == \"accident\":\n",
    "            columns_adj.remove('MAN_COL')\n",
    "            columns_adj.remove('NON_INVL')\n",
    "        if year !=  2010 and file_type != \"accident\":\n",
    "            columns_adj.remove(\"VEHNO\")\n",
    "        if year !=  2010 and file_type == \"person\":\n",
    "            columns_adj.remove('ALTRSULT')\n",
    "        if year !=  2010 and file_type == \"vehicle\":\n",
    "            columns_adj.remove(\"MODEL_YR\")\n",
    "            columns_adj.remove(\"DZIPCODE\")\n",
    "        if year !=  2010 and file_type == \"vevent\":\n",
    "            columns_adj.remove(\"GAD\")\n",
    "        \n",
    "        # read in current df and set columns appropriately \n",
    "        current_df = pd.read_sas(filepath)\n",
    "        if file_type == \"person\":\n",
    "            current_df = current_df.loc[current_df['PER_TYP'] == 1]\n",
    "        current_df = current_df[columns_adj]\n",
    "        \n",
    "        # rename column names appropriately for year and file type for joining\n",
    "        if year ==  2010 and file_type != \"accident\":\n",
    "            current_df = current_df.rename(columns = {\"VEHNO\" : \"VEH_NO\"})\n",
    "        if year ==  2010 and file_type == \"accident\": \n",
    "            current_df = current_df.rename(columns = {\"MAN_COL\" : \"MAN_COLL\", \"NON_INVL\" : \"PERNOTMVIT\"})\n",
    "        if file_type == \"person\":\n",
    "            current_df = current_df.rename(columns = {\"ALC_RES\" : \"ALTRSULT\"})\n",
    "        if year ==  2010 and file_type == \"vehicle\":\n",
    "            current_df = current_df.rename(columns = {\"MODEL_YR\" : \"MOD_YEAR\", \"DZIPCODE\" : \"DR_ZIP\"})\n",
    "        if year ==  2010 and file_type == \"vevent\":\n",
    "            current_df = current_df.rename(columns = {\"GAD\" : \"AOI1\"})\n",
    "        current_df[\"CASENUM\"] = current_df[\"CASENUM\"].astype(str)\n",
    "        if file_type != \"accident\":\n",
    "            current_df = current_df.dropna(subset = [\"CASENUM\", \"VEH_NO\"]) \n",
    "            current_df[\"VEH_NO\"] = current_df[\"VEH_NO\"].astype(str)\n",
    "            current_df[\"CASENUM_VEH_NO\"] = current_df[\"CASENUM\"] + current_df[\"VEH_NO\"]\n",
    "        else:\n",
    "            current_df = current_df.dropna(subset = [\"CASENUM\"])\n",
    "        #  for each file type, across years, add to container for concatenation\n",
    "        concat_file_type.append(current_df)\n",
    "        \n",
    "    # concatenate file type across each year\n",
    "    current_df = pd.concat(concat_file_type)\n",
    "    # delete data conatiner after concatenation\n",
    "    concat_file_type.clear()\n",
    "    # use a special way to join the accident file, which has no \n",
    "    # vehicle number, and therefore no attribute CASENUM_VEH_NO which was\n",
    "    # created for all other file types \n",
    "    if file_type == \"accident\":\n",
    "        distracted_df = distracted_df.drop(columns = [\"CASENUM_y\"])\n",
    "        distracted_df = pd.merge(left=distracted_df, right=current_df, left_on='CASENUM_x', right_on='CASENUM')\n",
    "    # merge each file type into single dataframe (already concatenated by year)\n",
    "    else:\n",
    "        distracted_df = pd.merge(left = distracted_df, right = current_df, left_on=\"CASENUM_VEH_NO\", right_on=\"CASENUM_VEH_NO\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform final operations to save data out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop duplicates along the CASENUM_VEH_NO column\n",
    "final_df = distracted_df.drop_duplicates(subset = \"CASENUM_VEH_NO\")\n",
    "# drop columns\n",
    "final_df = final_df.drop(columns = [\"CASENUM_x\", \"CASENUM_y\", \"VEH_NO_x\", \"VEH_NO_y\", \"PSUSTRAT\", \"PSU\", \"STRATUM\", \"PERNOTMVIT\"])\n",
    "# rename columns \n",
    "final_df = final_df.rename( columns = {\"MDRDSTRD\" : \"distraction\", \"PJ\" : \"police jurisdiction\", \"REGION\" : \"region\", \"MODEL\" : \"model\", \"MOD_YEAR\" : 'model year',\n",
    "        \"PER_TYP\" : \"person involved\", \"AGE\" : \"age\", \"SEX\" : \"sex\", \"ALTRSULT\" : \"alcohol\", \"DAY_WEEK\" : \"dow\", \"HOUR\" : \"hr\", \"INT_HWY\" : \"interstate hwy\", \"LAND_USE\" : \"population class\", \"MAN_COLL\" : \"collision\",\n",
    "        \"MAX_SEV\" : \"injury\", \"MINUTE\" : \"min\", \"MONTH\" : \"month\", \"RELJCT2\" : \"specific location\", \"REL_ROAD\" : \"relation to road\", \"TYP_INT\" : \"intersection type\", \"WRK_ZONE\" : \"work zone\", \"VNUM_LAN\" : \"lanes\", \"AOI1\" : \"area of impact\",\n",
    "        \"WEIGHT\" : \"weight\", \"TRAV_SP\" : \"speed\", \"VSPD_LIM\" : \"speed limit\", \"YEAR\": \"year\", \"ACC_TYPE\" : \"crash type\", \"DR_ZIP\" : \"zipcode\", \"MAKE\": \"make\", \"NUMOCCS\" : \"vehicle occupants\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe presents no need for dropping null items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r\"C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\2010_2015_GES_distracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murra667\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.read_csv(r'C:\\Users\\murra667\\Documents\\Springboard\\Capstone_2\\After 2010\\2010_2015_GES_distracted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"distracted\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df.loc[(final_df['distraction'] != 0.0) | (final_df['distraction'] == 96.0) | (final_df['distraction'] == 99.0), 'distracted'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "distracted = final_df.loc[final_df['distracted'] == 1]\n",
    "non_distracted = final_df.loc[final_df['distracted'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distracted Cases 98028\n",
      "Non-Distracted Cases 437992\n"
     ]
    }
   ],
   "source": [
    "print (\"Distracted Cases {}\\nNon-Distracted Cases {}\".format(len(distracted), len(non_distracted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
